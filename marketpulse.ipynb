{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "14fde24e",
      "metadata": {},
      "source": [
        "# üöÄ AI MarketPulse ‚Äì AI for Market Trend Analysis\n",
        "\n",
        "**Module E: AI Applications ‚Äì Individual Open Project**\n",
        "\n",
        "---\n",
        "\n",
        "This notebook presents a complete end-to-end AI system for analyzing retail/store sales data to:\n",
        "- üìà Identify market trends (Rising, Falling, Stable)\n",
        "- üîÆ Forecast future sales using Machine Learning\n",
        "- üí° Generate actionable business insights from pricing and discount patterns\n",
        "- ‚ö†Ô∏è Handle edge cases like cold-start products/stores\n",
        "\n",
        "**Author**: AI MarketPulse Team  \n",
        "**Date**: January 2026  \n",
        "**Version**: 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2921849",
      "metadata": {},
      "source": [
        "---\n",
        "# 1. Problem Definition & Objective\n",
        "\n",
        "## 1.1 Business Problem\n",
        "\n",
        "In today's competitive retail landscape, understanding market trends is crucial for business success. Retailers face several challenges:\n",
        "\n",
        "- **Inventory Management**: Overstocking leads to waste; understocking leads to lost sales\n",
        "- **Pricing Strategy**: Determining optimal prices and discount strategies\n",
        "- **Trend Detection**: Identifying which products are gaining or losing popularity\n",
        "- **Demand Forecasting**: Predicting future sales to plan operations\n",
        "\n",
        "## 1.2 Objectives\n",
        "\n",
        "This AI system aims to address these challenges through:\n",
        "\n",
        "1. **Trend Analysis**: Analyze historical sales data to identify rising, falling, and stable market trends\n",
        "2. **Sales Forecasting**: Build a machine learning model to predict future sales\n",
        "3. **Business Insights**: Generate actionable recommendations from pricing and discount patterns\n",
        "4. **Edge Case Handling**: Implement robust fallback strategies for new/cold-start products\n",
        "\n",
        "## 1.3 Approach\n",
        "\n",
        "We will use a combination of:\n",
        "- Statistical analysis for trend detection\n",
        "- Random Forest Regression for forecasting\n",
        "- Rule-based systems for insight generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc430f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS AND CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import requests\n",
        "from io import StringIO\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"AI MarketPulse - Market Trend Analysis System\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nLibraries loaded successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48be8870",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS FOR DATA LOADING\n",
        "# =============================================================================\n",
        "\n",
        "def generate_synthetic_data(n_records=5000):\n",
        "    \"\"\"\n",
        "    Generate synthetic retail sales data when real dataset is unavailable.\n",
        "    \"\"\"\n",
        "    print(\"Generating synthetic retail sales dataset...\")\n",
        "    \n",
        "    stores = [f'S{i:03d}' for i in range(1, 11)]  # 10 stores\n",
        "    products = [f'P{i:04d}' for i in range(1, 51)]  # 50 products\n",
        "    categories = ['Electronics', 'Clothing', 'Groceries', 'Home & Garden', 'Sports']\n",
        "    \n",
        "    start_date = datetime(2024, 1, 1)\n",
        "    end_date = datetime(2025, 12, 31)\n",
        "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "    \n",
        "    data = []\n",
        "    for _ in range(n_records):\n",
        "        date_idx = np.random.randint(0, len(date_range))\n",
        "        date = date_range[date_idx]\n",
        "        store = np.random.choice(stores)\n",
        "        product = np.random.choice(products)\n",
        "        category = np.random.choice(categories)\n",
        "        \n",
        "        base_prices = {'Electronics': 150, 'Clothing': 50, 'Groceries': 20, \n",
        "                       'Home & Garden': 80, 'Sports': 70}\n",
        "        price = base_prices[category] * np.random.uniform(0.5, 2.0)\n",
        "        \n",
        "        has_discount = np.random.random() < 0.3\n",
        "        discount = np.random.uniform(5, 30) if has_discount else 0\n",
        "        \n",
        "        month = date.month\n",
        "        seasonal_factor = 1.0 + 0.3 * np.sin(2 * np.pi * month / 12)\n",
        "        discount_factor = 1.0 + (discount / 100) * 0.5\n",
        "        price_factor = 100 / (price + 50)\n",
        "        \n",
        "        base_sales = np.random.poisson(20)\n",
        "        sales = int(base_sales * seasonal_factor * discount_factor * price_factor)\n",
        "        sales = max(1, sales)\n",
        "        \n",
        "        data.append({\n",
        "            'date': date,\n",
        "            'store_id': store,\n",
        "            'product_id': product,\n",
        "            'category': category,\n",
        "            'price': round(price, 2),\n",
        "            'discount': round(discount, 2),\n",
        "            'sales': sales\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"‚úì Generated {len(df)} synthetic records\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load retail sales data from public URL or generate synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DATA LOADING\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"\\n‚ö† Using synthetic data for reliable demo.\")\n",
        "    return generate_synthetic_data()\n",
        "\n",
        "print(\"‚úì Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29eb3bf5",
      "metadata": {},
      "source": [
        "---\n",
        "# 2. Data Understanding & Preparation\n",
        "\n",
        "## 2.1 Data Loading\n",
        "\n",
        "We will generate synthetic retail sales data with realistic patterns including:\n",
        "- Seasonal variations\n",
        "- Price-demand relationships\n",
        "- Discount effects on sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f322b46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD DATA\n",
        "# =============================================================================\n",
        "\n",
        "df_raw = load_data()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nShape: {df_raw.shape[0]} rows √ó {df_raw.shape[1]} columns\")\n",
        "print(f\"\\nColumn Names: {list(df_raw.columns)}\")\n",
        "print(f\"\\nData Types:\")\n",
        "print(df_raw.dtypes)\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "551da6e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPLORATORY DATA ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä Statistical Summary:\")\n",
        "print(df_raw.describe())\n",
        "\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "missing = df_raw.isnull().sum()\n",
        "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found!\")\n",
        "\n",
        "print(f\"\\nüìã Duplicate Rows: {df_raw.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f45418",
      "metadata": {},
      "source": [
        "## 2.2 Data Preprocessing\n",
        "\n",
        "The preprocessing pipeline includes:\n",
        "1. **Date Parsing**: Convert date strings to datetime objects\n",
        "2. **Duplicate Removal**: Remove exact duplicate rows\n",
        "3. **Missing Value Handling**: Fill or drop missing values appropriately\n",
        "4. **Outlier Capping**: Use IQR method to cap extreme values in sales\n",
        "5. **Feature Engineering**: Create time-based and lag features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f8b710",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA PREPROCESSING FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data preprocessing pipeline.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DATA PREPROCESSING\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    df = df.copy()\n",
        "    initial_rows = len(df)\n",
        "    \n",
        "    # Step 1: Parse date and sort\n",
        "    print(\"\\nüìÖ Step 1: Parsing dates and sorting chronologically...\")\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "    \n",
        "    # Step 2: Remove duplicates\n",
        "    print(\"\\nüîÑ Step 2: Removing duplicates...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"   Removed {initial_rows - len(df)} duplicate rows\")\n",
        "    \n",
        "    # Step 3: Handle missing values\n",
        "    print(\"\\n‚ùì Step 3: Handling missing values...\")\n",
        "    numeric_cols = ['price', 'discount', 'sales']\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            median_val = df[col].median()\n",
        "            df[col] = df[col].fillna(median_val)\n",
        "    df = df.dropna(subset=['date'])\n",
        "    print(f\"   Remaining rows: {len(df)}\")\n",
        "    \n",
        "    # Step 4: Cap outliers using IQR method\n",
        "    print(\"\\nüìä Step 4: Capping outliers using IQR method...\")\n",
        "    Q1 = df['sales'].quantile(0.25)\n",
        "    Q3 = df['sales'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers_before = ((df['sales'] < lower_bound) | (df['sales'] > upper_bound)).sum()\n",
        "    df['sales'] = df['sales'].clip(lower=max(0, lower_bound), upper=upper_bound)\n",
        "    print(f\"   IQR bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "    print(f\"   Capped {outliers_before} outlier values\")\n",
        "    \n",
        "    # Step 5: Add time features\n",
        "    print(\"\\nüïê Step 5: Adding time features...\")\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['week'] = df['date'].dt.isocalendar().week.astype(int)\n",
        "    df['dayofweek'] = df['date'].dt.dayofweek\n",
        "    print(\"   Added: year, month, week, dayofweek\")\n",
        "    \n",
        "    # Step 6: Add lag features\n",
        "    print(\"\\n‚èÆÔ∏è Step 6: Adding lag features...\")\n",
        "    df = df.sort_values(['store_id', 'product_id', 'date'])\n",
        "    for lag in [1, 2, 4]:\n",
        "        df[f'sales_lag_{lag}'] = df.groupby(['store_id', 'product_id'])['sales'].shift(lag)\n",
        "    print(\"   Added: sales_lag_1, sales_lag_2, sales_lag_4\")\n",
        "    \n",
        "    # Step 7: Add rolling features\n",
        "    print(\"\\nüìà Step 7: Adding rolling features...\")\n",
        "    df['sales_roll_mean_4'] = df.groupby(['store_id', 'product_id'])['sales'].transform(\n",
        "        lambda x: x.rolling(window=4, min_periods=1).mean())\n",
        "    df['sales_roll_std_4'] = df.groupby(['store_id', 'product_id'])['sales'].transform(\n",
        "        lambda x: x.rolling(window=4, min_periods=1).std())\n",
        "    df['sales_roll_std_4'] = df['sales_roll_std_4'].fillna(0)\n",
        "    print(\"   Added: sales_roll_mean_4, sales_roll_std_4\")\n",
        "    \n",
        "    # Fill remaining NaN in lag features\n",
        "    lag_cols = ['sales_lag_1', 'sales_lag_2', 'sales_lag_4']\n",
        "    df[lag_cols] = df[lag_cols].fillna(0)\n",
        "    \n",
        "    print(\"\\n‚úÖ Preprocessing complete!\")\n",
        "    print(f\"   Final shape: {df.shape}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"‚úì preprocess_data() function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83dbe982",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# APPLY PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "df = preprocess_data(df_raw)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PREPROCESSED DATA SAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nNew columns added: {[col for col in df.columns if col not in df_raw.columns]}\")\n",
        "print(f\"\\nSample of preprocessed data:\")\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347bad17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Sales distribution\n",
        "ax1 = axes[0, 0]\n",
        "ax1.hist(df['sales'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "ax1.set_title('Sales Distribution', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Sales')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.axvline(df['sales'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"sales\"].mean():.1f}')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Sales by Category\n",
        "ax2 = axes[0, 1]\n",
        "category_sales = df.groupby('category')['sales'].mean().sort_values(ascending=True)\n",
        "ax2.barh(category_sales.index, category_sales.values, color='teal', edgecolor='black')\n",
        "ax2.set_title('Average Sales by Category', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Average Sales')\n",
        "\n",
        "# Plot 3: Monthly Sales Trend\n",
        "ax3 = axes[1, 0]\n",
        "monthly_sales = df.groupby(df['date'].dt.to_period('M'))['sales'].sum()\n",
        "ax3.plot(monthly_sales.index.astype(str), monthly_sales.values, marker='o', \n",
        "         color='darkgreen', linewidth=2, markersize=4)\n",
        "ax3.set_title('Monthly Sales Trend', fontsize=12, fontweight='bold')\n",
        "ax3.set_xlabel('Month')\n",
        "ax3.set_ylabel('Total Sales')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 4: Price vs Sales Scatter\n",
        "ax4 = axes[1, 1]\n",
        "sample = df.sample(min(500, len(df)), random_state=42)\n",
        "ax4.scatter(sample['price'], sample['sales'], alpha=0.5, color='coral', edgecolor='black', s=30)\n",
        "ax4.set_title('Price vs Sales', fontsize=12, fontweight='bold')\n",
        "ax4.set_xlabel('Price')\n",
        "ax4.set_ylabel('Sales')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úì Data visualization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab38d8e",
      "metadata": {},
      "source": [
        "---\n",
        "# 3. Model / System Design\n",
        "\n",
        "## 3.1 System Architecture\n",
        "\n",
        "The AI MarketPulse system consists of three main modules:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    AI MARKETPULSE SYSTEM                        ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
        "‚îÇ  ‚îÇ   MODULE 1   ‚îÇ   ‚îÇ   MODULE 2   ‚îÇ   ‚îÇ   MODULE 3   ‚îÇ        ‚îÇ\n",
        "‚îÇ  ‚îÇ    TREND     ‚îÇ   ‚îÇ  FORECASTING ‚îÇ   ‚îÇ   INSIGHTS   ‚îÇ        ‚îÇ\n",
        "‚îÇ  ‚îÇ   ANALYSIS   ‚îÇ   ‚îÇ    MODEL     ‚îÇ   ‚îÇ  GENERATION  ‚îÇ        ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Monthly Aggregation    ‚Ä¢ RandomForest     ‚Ä¢ Price Correlation‚îÇ\n",
        "‚îÇ  ‚Ä¢ MoM Growth %           ‚Ä¢ Time-based Split ‚Ä¢ Discount Uplift ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Trend Classification   ‚Ä¢ MAE/RMSE/MAPE    ‚Ä¢ Business Insights‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## 3.2 Cold-Start Handling Strategy\n",
        "\n",
        "For products/stores with **fewer than 4 historical records**:\n",
        "- ‚ùå DO NOT use the ML forecasting model (insufficient data)\n",
        "- ‚úÖ Use **baseline prediction**: Store/Category average sales\n",
        "- üìù Log which groups used fallback method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "700bbc2d",
      "metadata": {},
      "source": [
        "---\n",
        "# 4. Core Implementation\n",
        "\n",
        "## 4.1 Trend Analysis Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50733d73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TREND ANALYSIS MODULE\n",
        "# =============================================================================\n",
        "\n",
        "def trend_analysis(df):\n",
        "    \"\"\"\n",
        "    Perform trend analysis on sales data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TREND ANALYSIS MODULE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Step 1: Aggregate monthly sales\n",
        "    print(\"\\nüìä Step 1: Aggregating monthly sales...\")\n",
        "    df['year_month'] = df['date'].dt.to_period('M')\n",
        "    \n",
        "    monthly_agg = df.groupby(['store_id', 'product_id', 'category', 'year_month']).agg({\n",
        "        'sales': 'sum', 'price': 'mean', 'discount': 'mean'\n",
        "    }).reset_index()\n",
        "    monthly_agg['year_month_str'] = monthly_agg['year_month'].astype(str)\n",
        "    print(f\"   Created {len(monthly_agg)} monthly aggregations\")\n",
        "    \n",
        "    # Step 2: Compute MoM growth\n",
        "    print(\"\\nüìà Step 2: Computing Month-over-Month growth...\")\n",
        "    monthly_agg = monthly_agg.sort_values(['store_id', 'product_id', 'year_month'])\n",
        "    monthly_agg['prev_sales'] = monthly_agg.groupby(['store_id', 'product_id'])['sales'].shift(1)\n",
        "    monthly_agg['mom_growth'] = ((monthly_agg['sales'] - monthly_agg['prev_sales']) / \n",
        "                                  monthly_agg['prev_sales'].replace(0, np.nan) * 100)\n",
        "    \n",
        "    # Step 3: Classify trends\n",
        "    print(\"\\nüè∑Ô∏è Step 3: Classifying trends...\")\n",
        "    def classify_trend(growth):\n",
        "        if pd.isna(growth): return 'Unknown'\n",
        "        elif growth > 5: return 'Rising'\n",
        "        elif growth < -5: return 'Falling'\n",
        "        else: return 'Stable'\n",
        "    \n",
        "    monthly_agg['trend'] = monthly_agg['mom_growth'].apply(classify_trend)\n",
        "    \n",
        "    trend_summary = monthly_agg.groupby(['store_id', 'product_id', 'category']).agg({\n",
        "        'mom_growth': 'mean', 'sales': 'sum'\n",
        "    }).reset_index()\n",
        "    trend_summary.columns = ['store_id', 'product_id', 'category', 'avg_growth', 'total_sales']\n",
        "    trend_summary['trend'] = trend_summary['avg_growth'].apply(classify_trend)\n",
        "    \n",
        "    trend_counts = trend_summary['trend'].value_counts()\n",
        "    print(\"\\n   Trend Distribution:\")\n",
        "    for trend, count in trend_counts.items():\n",
        "        print(f\"   ‚Ä¢ {trend}: {count} product-store combinations\")\n",
        "    \n",
        "    # Step 4: Top Rising and Falling\n",
        "    print(\"\\nüîù Step 4: Identifying top trends...\")\n",
        "    valid_trends = trend_summary[trend_summary['avg_growth'].notna()]\n",
        "    top_rising = valid_trends.nlargest(10, 'avg_growth')\n",
        "    top_falling = valid_trends.nsmallest(10, 'avg_growth')\n",
        "    \n",
        "    print(\"\\n\" + \"‚îÄ\" * 50)\n",
        "    print(\"üìà TOP 10 RISING PRODUCTS\")\n",
        "    print(\"‚îÄ\" * 50)\n",
        "    for _, row in top_rising.iterrows():\n",
        "        print(f\"   {row['store_id']}-{row['product_id']} ({row['category']}): +{row['avg_growth']:.1f}%\")\n",
        "    \n",
        "    print(\"\\n\" + \"‚îÄ\" * 50)\n",
        "    print(\"üìâ TOP 10 FALLING PRODUCTS\")\n",
        "    print(\"‚îÄ\" * 50)\n",
        "    for _, row in top_falling.iterrows():\n",
        "        print(f\"   {row['store_id']}-{row['product_id']} ({row['category']}): {row['avg_growth']:.1f}%\")\n",
        "    \n",
        "    # Step 5: Plot trend lines\n",
        "    print(\"\\nüìä Step 5: Plotting trend lines...\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Rising Trends\n",
        "    ax1 = axes[0]\n",
        "    colors_rising = plt.cm.Greens(np.linspace(0.4, 0.9, min(5, len(top_rising))))\n",
        "    for idx, (_, row) in enumerate(top_rising.head(5).iterrows()):\n",
        "        mask = ((monthly_agg['store_id'] == row['store_id']) & \n",
        "                (monthly_agg['product_id'] == row['product_id']))\n",
        "        item_data = monthly_agg[mask].sort_values('year_month')\n",
        "        if len(item_data) > 0:\n",
        "            ax1.plot(item_data['year_month_str'], item_data['sales'], \n",
        "                    marker='o', linewidth=2, markersize=4, color=colors_rising[idx],\n",
        "                    label=f\"{row['store_id']}-{row['product_id']} (+{row['avg_growth']:.1f}%)\")\n",
        "    ax1.set_title('üìà Top 5 Rising Trends', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Month')\n",
        "    ax1.set_ylabel('Sales')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    ax1.legend(loc='upper left', fontsize=8)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Falling Trends\n",
        "    ax2 = axes[1]\n",
        "    colors_falling = plt.cm.Reds(np.linspace(0.4, 0.9, min(5, len(top_falling))))\n",
        "    for idx, (_, row) in enumerate(top_falling.head(5).iterrows()):\n",
        "        mask = ((monthly_agg['store_id'] == row['store_id']) & \n",
        "                (monthly_agg['product_id'] == row['product_id']))\n",
        "        item_data = monthly_agg[mask].sort_values('year_month')\n",
        "        if len(item_data) > 0:\n",
        "            ax2.plot(item_data['year_month_str'], item_data['sales'],\n",
        "                    marker='o', linewidth=2, markersize=4, color=colors_falling[idx],\n",
        "                    label=f\"{row['store_id']}-{row['product_id']} ({row['avg_growth']:.1f}%)\")\n",
        "    ax2.set_title('üìâ Top 5 Falling Trends', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Month')\n",
        "    ax2.set_ylabel('Sales')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.legend(loc='upper right', fontsize=8)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return monthly_agg, trend_summary, top_rising, top_falling\n",
        "\n",
        "print(\"‚úì trend_analysis() function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ae47cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RUN TREND ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "monthly_agg, trend_summary, top_rising, top_falling = trend_analysis(df)\n",
        "\n",
        "print(\"\\n‚úÖ Trend analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f448919",
      "metadata": {},
      "source": [
        "## 4.2 Forecasting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2f889b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FORECASTING MODEL\n",
        "# =============================================================================\n",
        "\n",
        "def train_forecast_model(df, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Train a RandomForest model for sales forecasting.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FORECASTING MODEL\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    feature_cols = ['year', 'month', 'week', 'dayofweek', 'price', 'discount',\n",
        "                    'sales_lag_1', 'sales_lag_2', 'sales_lag_4',\n",
        "                    'sales_roll_mean_4', 'sales_roll_std_4']\n",
        "    target_col = 'sales'\n",
        "    \n",
        "    df_model = df.sort_values('date').reset_index(drop=True)\n",
        "    df_model = df_model.dropna(subset=feature_cols + [target_col])\n",
        "    \n",
        "    X = df_model[feature_cols]\n",
        "    y = df_model[target_col]\n",
        "    \n",
        "    # Time-based split (NO shuffle)\n",
        "    split_idx = int(len(df_model) * (1 - test_size))\n",
        "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "    \n",
        "    print(f\"\\nüìä Data Split:\")\n",
        "    print(f\"   Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "    print(f\"   Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "    \n",
        "    # Train model\n",
        "    print(\"\\nüå≤ Training RandomForestRegressor...\")\n",
        "    model = RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=5,\n",
        "                                   min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"   ‚úì Model trained successfully\")\n",
        "    \n",
        "    # Predictions and metrics\n",
        "    predictions = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    y_test_safe = np.where(y_test == 0, 1, y_test)\n",
        "    mape = np.mean(np.abs((y_test - predictions) / y_test_safe)) * 100\n",
        "    \n",
        "    metrics = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "    \n",
        "    print(\"\\nüìà Model Performance Metrics:\")\n",
        "    print(f\"   ‚Ä¢ MAE  (Mean Absolute Error): {mae:.2f}\")\n",
        "    print(f\"   ‚Ä¢ RMSE (Root Mean Squared Error): {rmse:.2f}\")\n",
        "    print(f\"   ‚Ä¢ MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
        "    \n",
        "    # Feature importance\n",
        "    print(\"\\nüéØ Top 5 Feature Importance:\")\n",
        "    importance = pd.DataFrame({'feature': feature_cols, 'importance': model.feature_importances_})\n",
        "    importance = importance.sort_values('importance', ascending=False)\n",
        "    for _, row in importance.head(5).iterrows():\n",
        "        print(f\"   ‚Ä¢ {row['feature']}: {row['importance']:.4f}\")\n",
        "    \n",
        "    return model, X_train, X_test, y_train, y_test, predictions, metrics, df_model\n",
        "\n",
        "print(\"‚úì train_forecast_model() function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2bca74f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN THE MODEL\n",
        "# =============================================================================\n",
        "\n",
        "model, X_train, X_test, y_train, y_test, predictions, metrics, df_model = train_forecast_model(df)\n",
        "\n",
        "print(\"\\n‚úÖ Model training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8827f9d",
      "metadata": {},
      "source": [
        "## 4.3 Cold-Start Handling with Fallback Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12fa4a8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COLD-START HANDLING - PREDICT WITH FALLBACK\n",
        "# =============================================================================\n",
        "\n",
        "def predict_with_fallback(df, model, min_records=4):\n",
        "    \"\"\"\n",
        "    Make predictions with fallback for cold-start products/stores.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"COLD-START HANDLING\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    feature_cols = ['year', 'month', 'week', 'dayofweek', 'price', 'discount',\n",
        "                    'sales_lag_1', 'sales_lag_2', 'sales_lag_4',\n",
        "                    'sales_roll_mean_4', 'sales_roll_std_4']\n",
        "    \n",
        "    group_counts = df.groupby(['store_id', 'product_id']).size().reset_index(name='record_count')\n",
        "    cold_start_groups = group_counts[group_counts['record_count'] < min_records]\n",
        "    normal_groups = group_counts[group_counts['record_count'] >= min_records]\n",
        "    \n",
        "    print(f\"\\nüìä Group Analysis:\")\n",
        "    print(f\"   Total unique store-product combinations: {len(group_counts)}\")\n",
        "    print(f\"   Groups with sufficient data (>= {min_records} records): {len(normal_groups)}\")\n",
        "    print(f\"   Cold-start groups (< {min_records} records): {len(cold_start_groups)}\")\n",
        "    \n",
        "    # Baselines\n",
        "    store_category_avg = df.groupby(['store_id', 'category'])['sales'].mean().to_dict()\n",
        "    category_avg = df.groupby('category')['sales'].mean().to_dict()\n",
        "    overall_avg = df['sales'].mean()\n",
        "    \n",
        "    results = []\n",
        "    fallback_groups = []\n",
        "    \n",
        "    for _, group_info in group_counts.iterrows():\n",
        "        store_id = group_info['store_id']\n",
        "        product_id = group_info['product_id']\n",
        "        record_count = group_info['record_count']\n",
        "        \n",
        "        mask = (df['store_id'] == store_id) & (df['product_id'] == product_id)\n",
        "        group_data = df[mask].copy()\n",
        "        \n",
        "        if record_count < min_records:\n",
        "            category = group_data['category'].iloc[0] if len(group_data) > 0 else 'Unknown'\n",
        "            baseline = store_category_avg.get((store_id, category), \n",
        "                       category_avg.get(category, overall_avg))\n",
        "            prediction = baseline\n",
        "            method = 'fallback_baseline'\n",
        "            fallback_groups.append({\n",
        "                'store_id': store_id, 'product_id': product_id, 'category': category,\n",
        "                'record_count': record_count, 'baseline_prediction': round(baseline, 2)\n",
        "            })\n",
        "        else:\n",
        "            latest_data = group_data.iloc[-1:].copy()\n",
        "            latest_data[feature_cols] = latest_data[feature_cols].fillna(0)\n",
        "            prediction = model.predict(latest_data[feature_cols])[0]\n",
        "            method = 'ml_model'\n",
        "        \n",
        "        results.append({\n",
        "            'store_id': store_id, 'product_id': product_id,\n",
        "            'predicted_sales': round(prediction, 2), 'method': method, 'record_count': record_count\n",
        "        })\n",
        "    \n",
        "    predictions_df = pd.DataFrame(results)\n",
        "    fallback_df = pd.DataFrame(fallback_groups)\n",
        "    \n",
        "    print(f\"\\nüìà Prediction Summary:\")\n",
        "    print(f\"   ML model predictions: {len(predictions_df[predictions_df['method'] == 'ml_model'])}\")\n",
        "    print(f\"   Baseline fallback predictions: {len(predictions_df[predictions_df['method'] == 'fallback_baseline'])}\")\n",
        "    \n",
        "    return predictions_df, fallback_df\n",
        "\n",
        "print(\"‚úì predict_with_fallback() function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067dd2f9",
      "metadata": {},
      "source": [
        "## 4.4 Business Insights Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b82add",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INSIGHTS GENERATION MODULE\n",
        "# =============================================================================\n",
        "\n",
        "def generate_insights(df, trend_summary):\n",
        "    \"\"\"\n",
        "    Generate business insights from pricing and discount patterns.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INSIGHTS GENERATION MODULE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    insights = {}\n",
        "    recommendations = []\n",
        "    \n",
        "    # Price-Sales Correlation\n",
        "    print(\"\\nüí∞ Analyzing Price-Sales Relationship...\")\n",
        "    price_corr = df['price'].corr(df['sales'])\n",
        "    insights['price_sales_correlation'] = round(price_corr, 4)\n",
        "    print(f\"   Price-Sales Correlation: {price_corr:.4f}\")\n",
        "    \n",
        "    if price_corr < -0.3:\n",
        "        recommendations.append(\"üìâ Strong negative correlation between price and sales. Consider strategic price reductions.\")\n",
        "    \n",
        "    # Discount-Sales Correlation\n",
        "    print(\"\\nüè∑Ô∏è Analyzing Discount-Sales Relationship...\")\n",
        "    discount_corr = df['discount'].corr(df['sales'])\n",
        "    insights['discount_sales_correlation'] = round(discount_corr, 4)\n",
        "    print(f\"   Discount-Sales Correlation: {discount_corr:.4f}\")\n",
        "    \n",
        "    # Discount Uplift\n",
        "    print(\"\\nüìä Calculating Discount Uplift...\")\n",
        "    df_with = df[df['discount'] > 0]\n",
        "    df_without = df[df['discount'] == 0]\n",
        "    avg_with = df_with['sales'].mean() if len(df_with) > 0 else 0\n",
        "    avg_without = df_without['sales'].mean() if len(df_without) > 0 else 0\n",
        "    uplift = ((avg_with - avg_without) / avg_without * 100) if avg_without > 0 else 0\n",
        "    \n",
        "    insights['avg_sales_with_discount'] = round(avg_with, 2)\n",
        "    insights['avg_sales_without_discount'] = round(avg_without, 2)\n",
        "    insights['discount_uplift_percent'] = round(uplift, 2)\n",
        "    \n",
        "    print(f\"   Average sales WITH discount: {avg_with:.2f}\")\n",
        "    print(f\"   Average sales WITHOUT discount: {avg_without:.2f}\")\n",
        "    print(f\"   Discount Uplift: {uplift:+.2f}%\")\n",
        "    \n",
        "    if uplift > 10:\n",
        "        recommendations.append(f\"‚úÖ Discounts effective! {uplift:.1f}% sales uplift. Maintain strategic discount campaigns.\")\n",
        "    \n",
        "    # Category Analysis\n",
        "    print(\"\\nüìÇ Analyzing Category Performance...\")\n",
        "    category_stats = df.groupby('category').agg({'sales': ['sum', 'mean', 'count']}).round(2)\n",
        "    category_stats.columns = ['total_sales', 'avg_sales', 'transaction_count']\n",
        "    category_stats = category_stats.sort_values('total_sales', ascending=False)\n",
        "    insights['category_stats'] = category_stats\n",
        "    \n",
        "    top_cat = category_stats.index[0]\n",
        "    bottom_cat = category_stats.index[-1]\n",
        "    recommendations.append(f\"üèÜ '{top_cat}' is top-performing. Allocate more inventory and marketing.\")\n",
        "    recommendations.append(f\"üîç '{bottom_cat}' shows lowest sales. Investigate causes.\")\n",
        "    \n",
        "    # Store Analysis\n",
        "    print(\"\\nüè™ Analyzing Store Performance...\")\n",
        "    store_stats = df.groupby('store_id').agg({'sales': ['sum', 'mean']}).round(2)\n",
        "    store_stats.columns = ['total_sales', 'avg_sales']\n",
        "    store_stats = store_stats.sort_values('total_sales', ascending=False)\n",
        "    insights['store_stats'] = store_stats\n",
        "    \n",
        "    top_store = store_stats.index[0]\n",
        "    bottom_store = store_stats.index[-1]\n",
        "    recommendations.append(f\"‚≠ê Store '{top_store}' is best performer. Replicate success factors.\")\n",
        "    recommendations.append(f\"üìã Store '{bottom_store}' underperforms. Consider operational review.\")\n",
        "    \n",
        "    # Trend insights\n",
        "    print(\"\\nüìà Generating Trend-Based Insights...\")\n",
        "    rising = len(trend_summary[trend_summary['trend'] == 'Rising'])\n",
        "    falling = len(trend_summary[trend_summary['trend'] == 'Falling'])\n",
        "    insights['trend_distribution'] = {'rising': rising, 'falling': falling}\n",
        "    \n",
        "    if rising > falling:\n",
        "        recommendations.append(f\"üåü Positive momentum: {rising} rising vs {falling} falling trends.\")\n",
        "    \n",
        "    # Seasonal insight\n",
        "    monthly_sales = df.groupby('month')['sales'].mean()\n",
        "    peak_month = monthly_sales.idxmax()\n",
        "    low_month = monthly_sales.idxmin()\n",
        "    month_names = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',\n",
        "                   7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
        "    recommendations.append(f\"üìÖ Peak month: {month_names[peak_month]}. Plan inventory buildup.\")\n",
        "    recommendations.append(f\"üìÖ Low month: {month_names[low_month]}. Consider promotions.\")\n",
        "    \n",
        "    insights['recommendations'] = recommendations\n",
        "    return insights\n",
        "\n",
        "print(\"‚úì generate_insights() function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21168d73",
      "metadata": {},
      "source": [
        "---\n",
        "# 5. Evaluation & Analysis\n",
        "\n",
        "## 5.1 Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca43f1cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL EVALUATION - ACTUAL VS PREDICTED\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä Performance Metrics Summary:\")\n",
        "print(\"‚îÄ\" * 40)\n",
        "print(f\"‚îÇ MAE  (Mean Absolute Error)      : {metrics['MAE']:.2f}\")\n",
        "print(f\"‚îÇ RMSE (Root Mean Squared Error)  : {metrics['RMSE']:.2f}\")\n",
        "print(f\"‚îÇ MAPE (Mean Absolute % Error)    : {metrics['MAPE']:.2f}%\")\n",
        "print(\"‚îÄ\" * 40)\n",
        "\n",
        "# Plot Actual vs Predicted\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.scatter(y_test, predictions, alpha=0.5, color='steelblue', edgecolor='black', s=30)\n",
        "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "ax1.set_title('Actual vs Predicted Sales', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Actual Sales')\n",
        "ax1.set_ylabel('Predicted Sales')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = axes[1]\n",
        "residuals = y_test.values - predictions\n",
        "ax2.hist(residuals, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
        "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "ax2.set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Residual (Actual - Predicted)')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úì Evaluation plots generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c43409c",
      "metadata": {},
      "source": [
        "## 5.2 Future Sales Forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dc6daf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FUTURE SALES FORECAST (NEXT 4 MONTHS)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FUTURE SALES FORECAST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "last_date = df['date'].max()\n",
        "print(f\"\\nLast date in dataset: {last_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "future_months = [last_date + pd.DateOffset(months=i) for i in range(1, 5)]\n",
        "print(f\"Forecasting for: {[d.strftime('%Y-%m') for d in future_months]}\")\n",
        "\n",
        "recent_data = df[df['date'] > (last_date - pd.DateOffset(months=3))].copy()\n",
        "forecast_base = recent_data.groupby(['store_id', 'category']).agg({\n",
        "    'price': 'mean', 'discount': 'mean', 'sales': 'mean',\n",
        "    'sales_lag_1': 'mean', 'sales_lag_2': 'mean', 'sales_lag_4': 'mean',\n",
        "    'sales_roll_mean_4': 'mean', 'sales_roll_std_4': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "forecasts = []\n",
        "for future_date in future_months:\n",
        "    for _, row in forecast_base.iterrows():\n",
        "        features = pd.DataFrame({\n",
        "            'year': [future_date.year], 'month': [future_date.month],\n",
        "            'week': [future_date.isocalendar()[1]], 'dayofweek': [future_date.dayofweek],\n",
        "            'price': [row['price']], 'discount': [row['discount']],\n",
        "            'sales_lag_1': [row['sales']], 'sales_lag_2': [row['sales_lag_1']],\n",
        "            'sales_lag_4': [row['sales_lag_2']], 'sales_roll_mean_4': [row['sales_roll_mean_4']],\n",
        "            'sales_roll_std_4': [row['sales_roll_std_4']]\n",
        "        })\n",
        "        predicted = model.predict(features)[0]\n",
        "        forecasts.append({'forecast_month': future_date.strftime('%Y-%m'),\n",
        "                          'store_id': row['store_id'], 'category': row['category'],\n",
        "                          'predicted_sales': round(max(0, predicted), 2)})\n",
        "\n",
        "forecast_df = pd.DataFrame(forecasts)\n",
        "monthly_forecast = forecast_df.groupby('forecast_month').agg({'predicted_sales': 'sum'}).reset_index()\n",
        "monthly_forecast.columns = ['Month', 'Predicted Total Sales']\n",
        "\n",
        "print(\"\\n\" + \"‚îÄ\" * 50)\n",
        "print(\"üìä MONTHLY FORECAST SUMMARY\")\n",
        "print(\"‚îÄ\" * 50)\n",
        "print(monthly_forecast.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "historical_monthly = df.groupby(df['date'].dt.to_period('M'))['sales'].sum()\n",
        "hist_dates = historical_monthly.index.astype(str)\n",
        "hist_values = historical_monthly.values\n",
        "\n",
        "ax.plot(hist_dates[-12:], hist_values[-12:], marker='o', color='steelblue', linewidth=2, markersize=6, label='Historical Sales')\n",
        "forecast_dates = monthly_forecast['Month'].values\n",
        "forecast_values = monthly_forecast['Predicted Total Sales'].values\n",
        "ax.plot([hist_dates[-1], forecast_dates[0]], [hist_values[-1], forecast_values[0]], color='coral', linewidth=2, linestyle='--')\n",
        "ax.plot(forecast_dates, forecast_values, marker='s', color='coral', linewidth=2, markersize=8, label='Forecast')\n",
        "ax.fill_between(forecast_dates, forecast_values * 0.85, forecast_values * 1.15, color='coral', alpha=0.2, label='Confidence Band (¬±15%)')\n",
        "ax.set_title('üìà Sales Forecast - Next 4 Months', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Month')\n",
        "ax.set_ylabel('Total Sales')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Forecast generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4517fdb",
      "metadata": {},
      "source": [
        "## 5.3 Pricing & Discount Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285b6c73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PRICING & DISCOUNT INSIGHTS\n",
        "# =============================================================================\n",
        "\n",
        "insights = generate_insights(df, trend_summary)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1 = axes[0]\n",
        "cat_stats = insights['category_stats']\n",
        "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(cat_stats)))\n",
        "bars = ax1.barh(cat_stats.index, cat_stats['total_sales'], color=colors, edgecolor='black')\n",
        "ax1.set_title('üìä Total Sales by Category', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Total Sales')\n",
        "for i, bar in enumerate(bars):\n",
        "    ax1.text(bar.get_width() + 50, bar.get_y() + bar.get_height()/2, f'{cat_stats[\"total_sales\"].iloc[i]:,.0f}', va='center', fontsize=9)\n",
        "\n",
        "ax2 = axes[1]\n",
        "categories = ['With Discount', 'Without Discount']\n",
        "values = [insights['avg_sales_with_discount'], insights['avg_sales_without_discount']]\n",
        "colors = ['coral', 'steelblue']\n",
        "bars = ax2.bar(categories, values, color=colors, edgecolor='black')\n",
        "ax2.set_title('üè∑Ô∏è Discount Impact on Average Sales', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Average Sales')\n",
        "for bar, val in zip(bars, values):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}', ha='center', fontsize=11, fontweight='bold')\n",
        "uplift = insights['discount_uplift_percent']\n",
        "ax2.annotate(f'Uplift: {uplift:+.1f}%', xy=(0.5, max(values) * 0.8), fontsize=12, fontweight='bold',\n",
        "             color='green' if uplift > 0 else 'red', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úì Pricing insights visualization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d4ea7b",
      "metadata": {},
      "source": [
        "## 5.4 Business Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "286c0031",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BUSINESS RECOMMENDATIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üìã BUSINESS RECOMMENDATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, rec in enumerate(insights['recommendations'], 1):\n",
        "    print(f\"\\n{i}. {rec}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä KEY METRICS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Price-Sales Correlation    : {insights['price_sales_correlation']:>8.4f}                      ‚îÇ\n",
        "‚îÇ  Discount-Sales Correlation : {insights['discount_sales_correlation']:>8.4f}                      ‚îÇ\n",
        "‚îÇ  Discount Uplift            : {insights['discount_uplift_percent']:>8.2f}%                     ‚îÇ\n",
        "‚îÇ  Avg Sales (with discount)  : {insights['avg_sales_with_discount']:>8.2f}                      ‚îÇ\n",
        "‚îÇ  Avg Sales (no discount)    : {insights['avg_sales_without_discount']:>8.2f}                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c0b6c0",
      "metadata": {},
      "source": [
        "## Edge Case Demonstration\n",
        "\n",
        "This section demonstrates the cold-start handling mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f072cb28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EDGE CASE DEMONSTRATION - COLD-START HANDLING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚ö†Ô∏è  EDGE CASE DEMONSTRATION: COLD-START HANDLING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüìù Creating cold-start scenario...\")\n",
        "\n",
        "cold_start_data = pd.DataFrame({\n",
        "    'date': pd.to_datetime(['2025-12-01', '2025-12-02', '2025-12-15']),\n",
        "    'store_id': ['S999', 'S999', 'S998'],\n",
        "    'product_id': ['P9999', 'P9999', 'P9998'],\n",
        "    'category': ['Electronics', 'Electronics', 'Clothing'],\n",
        "    'price': [150.0, 150.0, 45.0],\n",
        "    'discount': [10.0, 15.0, 0.0],\n",
        "    'sales': [5, 8, 3],\n",
        "    'year': [2025, 2025, 2025],\n",
        "    'month': [12, 12, 12],\n",
        "    'week': [49, 49, 50],\n",
        "    'dayofweek': [0, 1, 0],\n",
        "    'sales_lag_1': [0, 5, 0],\n",
        "    'sales_lag_2': [0, 0, 0],\n",
        "    'sales_lag_4': [0, 0, 0],\n",
        "    'sales_roll_mean_4': [5.0, 6.5, 3.0],\n",
        "    'sales_roll_std_4': [0.0, 2.1, 0.0]\n",
        "})\n",
        "\n",
        "df_with_cold_start = pd.concat([df, cold_start_data], ignore_index=True)\n",
        "print(f\"‚úì Added {len(cold_start_data)} cold-start records\")\n",
        "print(f\"   New product-store combinations: S999-P9999, S998-P9998\")\n",
        "\n",
        "predictions_df, fallback_df = predict_with_fallback(df_with_cold_start, model, min_records=4)\n",
        "\n",
        "print(\"\\n\" + \"‚îÄ\" * 70)\n",
        "print(\"üÜò GROUPS USING BASELINE FALLBACK (< 4 historical records)\")\n",
        "print(\"‚îÄ\" * 70)\n",
        "\n",
        "if len(fallback_df) > 0:\n",
        "    print(fallback_df.to_string(index=False))\n",
        "    print(\"\\n\" + \"‚îÄ\" * 70)\n",
        "    print(\"üìä FALLBACK PREDICTION DETAILS\")\n",
        "    print(\"‚îÄ\" * 70)\n",
        "    for _, row in fallback_df.iterrows():\n",
        "        print(f\"\\n   Store: {row['store_id']}, Product: {row['product_id']}\")\n",
        "        print(f\"   Category: {row['category']}\")\n",
        "        print(f\"   Historical Records: {row['record_count']}\")\n",
        "        print(f\"   ‚ùå ML Model: NOT USED (insufficient data)\")\n",
        "        print(f\"   ‚úÖ Baseline Prediction: {row['baseline_prediction']:.2f} units\")\n",
        "\n",
        "print(\"\\n\" + \"‚îÄ\" * 70)\n",
        "print(\"üìà PREDICTION METHOD SUMMARY\")\n",
        "print(\"‚îÄ\" * 70)\n",
        "method_counts = predictions_df['method'].value_counts()\n",
        "for method, count in method_counts.items():\n",
        "    pct = count / len(predictions_df) * 100\n",
        "    print(f\"   {method}: {count} groups ({pct:.1f}%)\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "methods = predictions_df.groupby('method')['predicted_sales'].mean()\n",
        "colors = ['steelblue' if m == 'ml_model' else 'coral' for m in methods.index]\n",
        "bars = ax.bar(methods.index, methods.values, color=colors, edgecolor='black')\n",
        "ax.set_title('üîÑ Average Predicted Sales by Method', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Average Predicted Sales')\n",
        "ax.set_xlabel('Prediction Method')\n",
        "for bar, val in zip(bars, methods.values):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}', ha='center', fontsize=11, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Edge case demonstration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5575e1",
      "metadata": {},
      "source": [
        "---\n",
        "# 6. Ethical Considerations & Responsible AI\n",
        "\n",
        "## 6.1 Bias and Fairness Risks\n",
        "\n",
        "**Potential Biases:**\n",
        "- **Regional Bias**: Model may perform better for stores with more historical data\n",
        "- **Category Bias**: Categories with more data may have more accurate predictions\n",
        "- **Temporal Bias**: Seasonal patterns may not generalize to unusual years\n",
        "\n",
        "## 6.2 Dataset Limitations\n",
        "\n",
        "- Synthetic data may not capture all real-world complexities\n",
        "- Historical patterns may not predict unprecedented events\n",
        "- External factors (economic conditions, competitors) not included\n",
        "\n",
        "## 6.3 Privacy Compliance\n",
        "\n",
        "‚úÖ **This system does NOT use any personal customer data.**\n",
        "\n",
        "## 6.4 Responsible Use Guidelines\n",
        "\n",
        "- Use predictions as one input among many for business decisions\n",
        "- Regularly validate model performance against actual outcomes\n",
        "- Do not make critical decisions solely based on AI predictions\n",
        "\n",
        "## 6.5 AI Tools Disclosure\n",
        "\n",
        "This project was developed using Python, pandas, NumPy, scikit-learn, and matplotlib. GitHub Copilot assisted in code development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a20970",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BIAS ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîç BIAS ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüìä Model Performance by Category:\")\n",
        "print(\"‚îÄ\" * 50)\n",
        "\n",
        "test_indices = df_model.index[int(len(df_model) * 0.8):]\n",
        "test_data = df_model.loc[test_indices].copy()\n",
        "test_data['predicted'] = predictions\n",
        "\n",
        "category_performance = test_data.groupby('category').apply(\n",
        "    lambda x: pd.Series({\n",
        "        'count': len(x),\n",
        "        'actual_mean': x['sales'].mean(),\n",
        "        'predicted_mean': x['predicted'].mean(),\n",
        "        'mae': mean_absolute_error(x['sales'], x['predicted']),\n",
        "        'mape': np.mean(np.abs((x['sales'] - x['predicted']) / x['sales'].replace(0, 1))) * 100\n",
        "    })\n",
        ").round(2)\n",
        "\n",
        "print(category_performance)\n",
        "\n",
        "max_mape = category_performance['mape'].max()\n",
        "min_mape = category_performance['mape'].min()\n",
        "disparity = max_mape - min_mape\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è Fairness Check:\")\n",
        "print(f\"   MAPE range across categories: {min_mape:.1f}% - {max_mape:.1f}%\")\n",
        "print(f\"   Disparity: {disparity:.1f} percentage points\")\n",
        "\n",
        "if disparity > 20:\n",
        "    print(\"   ‚ö†Ô∏è WARNING: Significant performance disparity detected.\")\n",
        "else:\n",
        "    print(\"   ‚úÖ Performance is relatively consistent across categories.\")\n",
        "\n",
        "print(\"\\n‚úì Bias analysis complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16b83287",
      "metadata": {},
      "source": [
        "---\n",
        "# 7. Conclusion & Future Scope\n",
        "\n",
        "## 7.1 Summary of Findings\n",
        "\n",
        "This AI MarketPulse system successfully demonstrates:\n",
        "\n",
        "1. **Data Pipeline**: Robust preprocessing with outlier handling and feature engineering\n",
        "2. **Trend Analysis**: Accurate identification of rising, falling, and stable trends\n",
        "3. **Sales Forecasting**: Random Forest model with reasonable accuracy\n",
        "4. **Business Insights**: Automated generation of actionable recommendations\n",
        "5. **Cold-Start Handling**: Graceful fallback for new products/stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1777c3d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY AND RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä FINAL RESULTS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    AI MARKETPULSE RESULTS                          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  üìà TREND ANALYSIS                                                 ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Rising Trends:  {insights['trend_distribution']['rising']:>4} product-store combinations            ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Falling Trends: {insights['trend_distribution']['falling']:>4} product-store combinations            ‚îÇ\n",
        "‚îÇ                                                                    ‚îÇ\n",
        "‚îÇ  üîÆ FORECASTING MODEL                                              ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Algorithm: RandomForestRegressor                                ‚îÇ\n",
        "‚îÇ  ‚Ä¢ MAE:  {metrics['MAE']:>8.2f}                                             ‚îÇ\n",
        "‚îÇ  ‚Ä¢ RMSE: {metrics['RMSE']:>8.2f}                                             ‚îÇ\n",
        "‚îÇ  ‚Ä¢ MAPE: {metrics['MAPE']:>8.2f}%                                            ‚îÇ\n",
        "‚îÇ                                                                    ‚îÇ\n",
        "‚îÇ  üí° KEY INSIGHTS                                                   ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Price-Sales Correlation:    {insights['price_sales_correlation']:>8.4f}                       ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Discount Uplift:            {insights['discount_uplift_percent']:>8.2f}%                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"‚îÄ\" * 70)\n",
        "print(\"üìã TOP 5 RISING TRENDS\")\n",
        "print(\"‚îÄ\" * 70)\n",
        "print(top_rising[['store_id', 'product_id', 'category', 'avg_growth', 'total_sales']].head().to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"‚îÄ\" * 70)\n",
        "print(\"üìã TOP 5 FALLING TRENDS\")\n",
        "print(\"‚îÄ\" * 70)\n",
        "print(top_falling[['store_id', 'product_id', 'category', 'avg_growth', 'total_sales']].head().to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33aa46d5",
      "metadata": {},
      "source": [
        "## 7.2 Known Limitations\n",
        "\n",
        "| Limitation | Impact | Mitigation |\n",
        "|------------|--------|------------|\n",
        "| Synthetic data | May not capture real-world complexity | Integrate real datasets |\n",
        "| No external factors | Cannot predict market disruptions | Add economic indicators |\n",
        "| Cold-start accuracy | Baseline predictions less precise | Collaborative filtering |\n",
        "\n",
        "## 7.3 Future Scope\n",
        "\n",
        "1. **Deep Learning Models**: LSTM for complex temporal patterns\n",
        "2. **External Data**: Economic indicators, weather, social media\n",
        "3. **Real-time Streaming**: Live data processing\n",
        "4. **Automated Alerting**: Notify stakeholders of trend changes\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for reviewing AI MarketPulse!**\n",
        "\n",
        "*Module E: AI Applications ‚Äì Individual Open Project*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "350bed72",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NOTEBOOK EXECUTION COMPLETE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéâ AI MARKETPULSE NOTEBOOK EXECUTION COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "All sections executed successfully:\n",
        "  ‚úÖ 1. Problem Definition & Objective\n",
        "  ‚úÖ 2. Data Understanding & Preparation  \n",
        "  ‚úÖ 3. Model / System Design\n",
        "  ‚úÖ 4. Core Implementation\n",
        "  ‚úÖ 5. Evaluation & Analysis\n",
        "  ‚úÖ 6. Ethical Considerations & Responsible AI\n",
        "  ‚úÖ 7. Conclusion & Future Scope\n",
        "\n",
        "Thank you for using AI MarketPulse!\n",
        "\"\"\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
